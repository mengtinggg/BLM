{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "\n",
    "dict_tagged_sentences = ''\n",
    "# Below indicates the relative path to\n",
    "# positive/negative/inverter/incrementer/decrementer files\n",
    "DICTIONARY_DIR_PREFIX = 'dicts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Splitter(object):\n",
    "    def __init__(self):\n",
    "        self.nltk_splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        self.nltk_tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "    def split(self, text):\n",
    "        \"\"\"\n",
    "        input format: a paragraph of text\n",
    "        output format: a list of lists of words.\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        \"\"\"\n",
    "        sentences = self.nltk_splitter.tokenize(text)\n",
    "        tokenized_sentences = [self.nltk_tokenizer.tokenize(sent) for sent in sentences]\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "class POSTagger(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def pos_tag(self, sentences):\n",
    "        \"\"\"\n",
    "        input format: list of lists of words\n",
    "            e.g.: [['this', 'is', 'a', 'sentence'], ['this', 'is', 'another', 'one']]\n",
    "        output format: list of lists of tagged tokens. Each tagged tokens has a\n",
    "        form, a lemma, and a list of tags\n",
    "            e.g: [[('this', 'this', ['DT']), ('is', 'be', ['VB']), ('a', 'a', ['DT']), ('sentence', 'sentence', ['NN'])],\n",
    "                    [('this', 'this', ['DT']), ('is', 'be', ['VB']), ('another', 'another', ['DT']), ('one', 'one', ['CARD'])]]\n",
    "        \"\"\"\n",
    "\n",
    "        pos = [nltk.pos_tag(sentence) for sentence in sentences]\n",
    "        #adapt format\n",
    "        pos = [[(word, word, [postag]) for (word, postag) in sentence] for sentence in pos]\n",
    "        return pos\n",
    "\n",
    "class DictionaryTagger(object):\n",
    "    def __init__(self, dictionary_paths):\n",
    "        \"\"\"\n",
    "\n",
    "        :rtype : object\n",
    "        \"\"\"\n",
    "        files = [open(path, 'r') for path in dictionary_paths]\n",
    "        dictionaries = [yaml.load(dict_file) for dict_file in files]\n",
    "        map(lambda x: x.close(), files)\n",
    "        self.dictionary = {}\n",
    "        self.max_key_size = 0\n",
    "        for curr_dict in dictionaries:\n",
    "            for key in curr_dict:\n",
    "                if key in self.dictionary:\n",
    "                    self.dictionary[key].extend(curr_dict[key])\n",
    "                else:\n",
    "                    self.dictionary[key] = curr_dict[key]\n",
    "                    self.max_key_size = max(self.max_key_size, len(key))\n",
    "\n",
    "    def tag(self, postagged_sentences):\n",
    "        return [self.tag_sentence(sentence) for sentence in postagged_sentences]\n",
    "\n",
    "    def tag_sentence(self, sentence, tag_with_lemmas=False):\n",
    "        \"\"\"\n",
    "        the result is only one tagging of all the possible ones.\n",
    "        The resulting tagging is determined by these two priority rules:\n",
    "            - longest matches have higher priority\n",
    "            - search is made from left to right\n",
    "        \"\"\"\n",
    "        tag_sentence = []\n",
    "        N = len(sentence)\n",
    "        if self.max_key_size == 0:\n",
    "            self.max_key_size = N\n",
    "        i = 0\n",
    "        while (i < N):\n",
    "            j = min(i + self.max_key_size, N) #avoid overflow\n",
    "            tagged = False\n",
    "            while (j > i):\n",
    "                expression_form = ' '.join([word[0] for word in sentence[i:j]]).lower()\n",
    "                expression_lemma = ' '.join([word[1] for word in sentence[i:j]]).lower()\n",
    "                if tag_with_lemmas:\n",
    "                    literal = expression_lemma\n",
    "                else:\n",
    "                    literal = expression_form\n",
    "                if literal in self.dictionary:\n",
    "                    #self.logger.debug(\"found: %s\" % literal)\n",
    "                    is_single_token = j - i == 1\n",
    "                    original_position = i\n",
    "                    i = j\n",
    "                    taggings = [tag for tag in self.dictionary[literal]]\n",
    "                    tagged_expression = (expression_form, expression_lemma, taggings)\n",
    "                    if is_single_token: #if the tagged literal is a single token, conserve its previous taggings:\n",
    "                        original_token_tagging = sentence[original_position][2]\n",
    "                        tagged_expression[2].extend(original_token_tagging)\n",
    "                    tag_sentence.append(tagged_expression)\n",
    "                    tagged = True\n",
    "                else:\n",
    "                    j = j - 1\n",
    "            if not tagged:\n",
    "                tag_sentence.append(sentence[i])\n",
    "                i += 1\n",
    "        return tag_sentence\n",
    "\n",
    "def value_of(sentiment):\n",
    "    if sentiment == 'positive': return 1\n",
    "    if sentiment == 'negative': return -1\n",
    "    return 0\n",
    "\n",
    "def sentiment_score(review):\n",
    "    return sum ([value_of(tag) for sentence in dict_tagged_sentences for token in sentence for tag in token[2]])\n",
    "\n",
    "def sentence_score(sentence_tokens, previous_token, acum_score):\n",
    "    if not sentence_tokens:\n",
    "        return acum_score\n",
    "    else:\n",
    "        current_token = sentence_tokens[0]\n",
    "        tags = current_token[2]\n",
    "        token_score = sum([value_of(tag) for tag in tags])\n",
    "        if previous_token is not None:\n",
    "            previous_tags = previous_token[2]\n",
    "            if 'inc' in previous_tags:\n",
    "                token_score *= 2.0\n",
    "            elif 'dec' in previous_tags:\n",
    "                token_score /= 2.0\n",
    "            elif 'inv' in previous_tags:\n",
    "                token_score *= -1.0\n",
    "        return sentence_score(sentence_tokens[1:], current_token, acum_score + token_score)\n",
    "\n",
    "def sentiment_score(sentences):\n",
    "    return sum([sentence_score(sentence, None, 0.0) for sentence in sentences])\n",
    "\n",
    "\n",
    "def run_analysis(text):\n",
    "    splitter = Splitter() # This boy will split a long single string into sentences.\n",
    "    postagger = POSTagger() # This boy is the Part-Of-Speech tagger.\n",
    "\n",
    "    # If text contains multiple sentences, this line splits it into individual sentences.\n",
    "    splitted_sentences = splitter.split(text)\n",
    "    print (splitted_sentences)\n",
    "    #exit(1)\n",
    "\n",
    "    # print (\"########## This performs Part-Of-Speech tagging. ##########\")\n",
    "    # This performs Part-Of-Speech tagging.\n",
    "    pos_tagged_sentences = postagger.pos_tag(splitted_sentences)\n",
    "    # pprint (pos_tagged_sentences)\n",
    "    # exit(1)\n",
    "\n",
    "    # print (\"########## This line loads Positive word and Negative word corpus. ##########\")\n",
    "    # This line loads Positive word and Negative word dictionaries.\n",
    "    dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml'])\n",
    "    dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    # pprint(dict_tagged_sentences)\n",
    "    # exit(1)\n",
    "\n",
    "    # print (\"########## [Baseline Analysis] Using only Positive/Negative corpus. ##########\")\n",
    "    score = sentiment_score(dict_tagged_sentences)\n",
    "    # print (\"Score: %d\" % score)\n",
    "    #exit(1)\n",
    "\n",
    "    # print (\"########## This line loads Positve/Negative corpus + incrementer/decrementer corpus. ##########\")\n",
    "    # dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml', DICTIONARY_DIR_PREFIX + 'inc.yml', DICTIONARY_DIR_PREFIX + 'dec.yml'])\n",
    "    # dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    # #pprint(dict_tagged_sentences)\n",
    "    # score = sentiment_score(dict_tagged_sentences)\n",
    "    # print (\"Score: %d\" % score)\n",
    "    # #exit(1)\n",
    "\n",
    "    # print (\"########## This line loads Positve/Negative corpus + incrementer/decrementer/inverter corpus. ##########\")\n",
    "    # dicttagger = DictionaryTagger([ DICTIONARY_DIR_PREFIX + 'positive.yml', DICTIONARY_DIR_PREFIX + 'negative.yml', DICTIONARY_DIR_PREFIX + 'inc.yml', DICTIONARY_DIR_PREFIX + 'dec.yml', DICTIONARY_DIR_PREFIX + 'inv.yml'])\n",
    "    # dict_tagged_sentences = dicttagger.tag(pos_tagged_sentences)\n",
    "    # #pprint(dict_tagged_sentences)\n",
    "    # score = sentiment_score(dict_tagged_sentences)\n",
    "    # print (\"Score: %d\" % score)\n",
    "    return score\n",
    "    #exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of data containing reparation : 208\n"
    }
   ],
   "source": [
    "######## Import Data ###########\n",
    "file_paths = [\"Reddit2015andBefore.csv\", \"Reddit2018to2015.csv\", \"Reddit2019.csv\", \"RedditJan2020_July2020.csv\"]\n",
    "\n",
    "data_list = []\n",
    "\n",
    "keyword = \"\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "for path in file_paths:\n",
    "    dataframe = pd.read_csv(path, index_col = False)\n",
    "\n",
    "    title_list = dataframe[\"title\"].tolist()\n",
    "\n",
    "    for title in title_list:\n",
    "        if keyword in title:\n",
    "            data_list += [\" \".join(tokenizer.tokenize(title.lower()))]\n",
    "\n",
    "    for comment in dataframe[\"comments\"].tolist():\n",
    "        if type(comment) != float :\n",
    "            comment_list = comment.split(\"\\n\\n\")[:-1]\n",
    "            for el in comment_list:\n",
    "                if keyword in el:\n",
    "                    data_list += [\" \".join(tokenizer.tokenize(el.lower()))]\n",
    "\n",
    "print(\"Number of data containing\",keyword,\":\",len(data_list))\n",
    "# print(data_list[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ", 'right', 'next', 'to', 'you', 'does', 'and', 'stop', 'supporting', 'a', 'regime', 'that', 'originates', 'from', 'a', 'cia', 'backed', 'dictatorship', 'defund', 'the', 'police', 'and', 'not', 'have', 'to', 'bash', 'anyone', 's', 'skull', 'in']]\n[['then', 'better', 'train', 'them', 'instead', 'of', 'calling', 'them', 'horrible', 'and', 'trying', 'to', 'defund', 'them']]\n[['who', 'raises', 'your', 'taxes', 'who', 'avoids', 'offering', 'common', 'people', 'bailouts', 'who', 'makes', 'it', 'expensive', 'to', 'create', 'a', 'daycare', 'which', 'makes', 'your', 'childcare', 'more', 'expensive', 'and', 'then', 'gives', 'your', 'children', 'subpar', 'education', 'as', 'they', 'defund', 'the', 'schools', 'who', 'makes', 'you', 'pay', 'annually', 'for', 'your', 'car', 'your', 'house', 'your', 'ability', 'to', 'run', 'a', 'business', 'every', 'step', 'you', 'take', 'to', 'be', 'successful', 'there', 's', 'some', 'rich', 'asshole', 'with', 'his', 'hand', 'in', 'your', 'pocket', 'taking', 'your', 'hard', 'earned', 'wages', 'into', 'his', 'hands']]\n[['bring', 'our', 'soldiers', 'home', 'it', 's', 'time', 'to', 'restructure', 'and', 'defund', 'our', 'military', 'it', 's', 'time', 'to', 'restructure', 'our', 'police', 'it', 's', 'time', 'to', 'restructure', 'our', 'economy', 'it', 's', 'time', 'to', 'restructure', 'our', 'country', 'we', 'are', 'all', 'citizens', 'of', 'america', 'it', 's', 'time', 'we', 'act', 'like', 'it', 'and', 'fight', 'for', 'a', 'country', 'we', 'all', 'love', 'to', 'live', 'in']]\n[['i', 'm', 'a', '32', 'year', 'old', 'white', 'male', 'from', 'a', 'small', 'texas', 'town', 'in', 'a', 'predominantly', 'conservative', 'area', 'ive', 'been', 'constantly', 'in', 'a', 'state', 'of', 'stress', 'and', 'near', 'tears', 'on', 'a', 'daily', 'basis', 'since', 'all', 'of', 'this', 'has', 'started', 'to', 'be', 'clear', 'i', '100', 'support', 'black', 'lives', 'matters', 'and', 'i', 'm', 'just', 'overwhelmed', 'with', 'finally', 'seeing', 'things', 'move', 'in', 'a', 'positive', 'direction', 'communities', 'moving', 'toward', 'defunding', 'the', 'police', 'and', 'reinvesting', 'back', 'into', 'the', 'community', 'is', 'one', 'of', 'the', 'most', 'important', 'things', 'for', 'me']]\n[['when', 'the', 'blm', 'protests', 'started', 'my', 'boss', 'asked', 'me', 'to', 'draft', 'a', 'blm', 'resolution', 'and', 'i', 'was', 'super', 'excited', 'but', 'i', 'kept', 'proposing', 'language', 'that', 'would', 'essentially', 'signify', 'a', 'commitment', 'to', 'defund', 'the', 'police', 'or', 'do', 'anything', 'to', 'help', 'and', 'he', 'kept', 'removing', 'it']]\n[['eventually', 'i', 'asked', 'what', 'exactly', 'he', 'wanted', 'and', 'he', 'essentially', 'said', 'he', 'just', 'wants', 'to', 'appease', 'the', 'masses', 'and', 'make', 'it', 'all', 'go', 'away', 'i', 'tried', 'to', 'convince', 'him', 'otherwise', 'but', 'he', 'brushed', 'me', 'off', 'the', 'resolution', 'went', 'up', 'for', 'a', 'vote', 'yesterday', 'with', 'no', 'defund', 'language', 'thousands', 'of', 'people', 'called', 'in', 'and', 'asked', 'for', 'a', 'stronger', 'commitment', 'he', 'ignored', 'them', 'the', 'legislative', 'body', 'even', 'approve', 'an', 'increased', 'police', 'budget', 'in', 'the', 'same', 'meeting']]\n[['i', 'hate', 'police', 'so', 'so', 'much', 'i', 'hate', 'every', 'arguement', 'rhats', 'pro', 'cop', 'ive', 'never', 'seen', 'one', 'that', 'isnt', 'its', 'orders', 'theyre', 'people', 'too', 'its', 'just', 'some', 'bad', 'apples', 'etc', 'they', 'are', 'people', 'so', 'are', 'the', 'people', 'theyre', 'fucking', 'murdering', 'because', 'thats', 'what', 'theyre', 'doing', 'murdering', 'they', 'are', 'slaughtering', 'human', 'beings', 'for', 'the', 'skinnthey', 'were', 'born', 'with', 'a', 'blue', 'life', 'is', 'not', 'a', 'life', 'it', 'is', 'a', 'shirt', 'they', 'can', 'take', 'it', 'off', 'a', 'black', 'man', 'can', 'not', 'take', 'off', 'his', 'skin', 'a', 'blue', 'shirt', 'may', 'have', 'to', 'be', 'scared', 'of', 'criminals', 'while', 'in', 'the', 'shirt', 'but', 'a', 'black', 'person', 'has', 'to', 'be', 'scared', 'of', 'law', 'enforcement', 'every', 'waking', 'second', 'irs', 'disgusting', 'vile', 'and', 'they', 'should', 'be', 'defunded', 'and', 'rebuilt', 'i', 'dont', 'understand', 'what', 'itll', 'take', 'for', 'people', 'to', 'get', 'their', 'heads', 'out', 'of', 'their', 'fucking', 'asses', 'children', 'dying', 'oh', 'wait', 'they', 'are', 'children', 'are', 'dying', 'humans', 'are', 'dying', 'people', 'are', 'being', 'fucking', 'killed', 'hecause', 'of', 'a', 'power', 'trip', 'i', 'hate', 'police', 'even', 'if', 'im', 'white', 'i', 'will', 'never', 'trust', 'any', 'of', 'them', 'a', 'few', 'bad', 'apples', 'spoils', 'the', 'bunch', 'and', 'the', 'non', 'bad', 'apples', 'are', 'enabling', 'it', 'bu', 'working', 'for', 'a', 'racist', 'oppressive', 'system', 'i', 'am', 'seething', 'every', 'moment', 'i', 'think', 'about', 'it', 'i', 'hope', 'these', 'people', 'have', 'children', 'and', 'grandchildren', 'and', 'family', 'who', 'are', 'ashamed', 'of', 'them', 'i', 'hope', 'that', '50', 'years', 'from', 'now', 'the', 'descendants', 'of', 'these', 'murderers', 'will', 'be', 'embarrassed', 'of', 'their', 'bloodline', 'because', 'of', 'these', 'killers', 'if', 'youre', 'a', 'cop', 'quit', 'your', 'job', 'thats', 'thhe', 'only', 'way', 'to', 'make', 'it', 'right', 'get', 'rid', 'of', 'police', 'defund', 'them', 'take', 'licenses', 'away', 'stop', 'killing', 'people', 'for', 'peacefully', 'protesting', 'stop', 'hurting', 'people', 'for', 'just', 'existing', 'i', 'hate', 'living', 'in', 'america', 'i', 'hate', 'this', 'entire', 'country', 'and', 'i', 'want', 'to', 'leave', 'badly']]\n[['the', 'lack', 'of', 'organization', 'and', 'a', 'figure', 'head', 'in', 'the', 'blm', 'movement', 'will', 'halt', 'its', 'progress', 'it', 'needs', 'organization', 'it', 'needs', 'groups', 'it', 'i', 'personally', 'feel', 'needs', 'to', 'operate', 'like', 'a', 'corporation', 'a', 'group', 'that', 'protests', 'sure', 'but', 'they', 'also', 'need', 'people', 'with', 'knowledge', 'of', 'the', 'legal', 'side', 'of', 'things', 'laws', 'need', 'to', 'be', 'read', 'and', 'changed', 'the', 'medical', 'side', 'of', 'things', 'this', 'is', 'draining', 'not', 'just', 'for', 'poc', 'and', 'allies', 'but', 'some', 'white', 'people', 'and', 'poc', 'have', 'been', 'coming', 'to', 'terms', 'with', 'their', 'prejudices', 'to', 'eachother', 'and', 'themselves', 'they', 'are', 'finding', 'so', 'much', 'of', 'what', 'they', 'know', 'are', 'lies', 'so', 'they', 'become', 'reactionary', 'i', 'see', 'no', 'reason', 'why', 'racism', 'can', 'not', 'be', 'treated', 'like', 'terrorism', 'where', 'you', 'deprogram', 'them', 'they', 'need', 'people', 'for', 'that', 'places', 'for', 'that', 'it', 'needs', 'to', 'operate', 'like', 'a', 'powerful', 'machine', 'that', 'does', 'steamroll', 'without', 'reason', 'people', 'are', 'not', 'thinking', 'in', 'the', 'long', 'term', 'it', 'worries', 'me', 'that', 'the', 'goal', 'will', 'be', 'lost', 'i', 'secretly', 'had', 'the', 'phrase', 'defund', 'the', 'police', 'primarily', 'because', 'i', 'belive', 'slogans', 'should', 'not', 'need', 'an', 'explanation', 'it', 'should', 'be', 'understood', 'just', 'by', 'reading', 'it', 'once', 'defund', 'has', 'a', 'definition', 'as', 'does', 'the', 'term', 'reform', 'agitate', 'as', 'we', 'ameliorate']]\n[['lastly', 'black', 'lives', 'fucking', 'matter', 'defund', 'the', 'police', 'all', 'lives', 'don', 't', 'matter', 'until', 'black', 'lives', 'do', 'think', 'otherwise', 'you', 're', 'racist', 'i', 'have', 'gone', 'to', 'many', 'protests', 'and', 'i', 'feel', 'the', 'love', 'and', 'energy', 'the', 'black', 'community', 'radiates', 'the', 'love', 'and', 'compassion', 'we', 'all', 'have', 'for', 'them', 'i', 'will', 'never', 'stop', 'supporting', 'you', 'fuck', 'if', 'i', 'do', 'im', 'dead']]\n[['i', 've', 'been', 'ignoring', 'bringing', 'up', 'the', 'protests', 'with', 'them', 'i', 'know', 'they', 'are', 'well', 'aware', 'and', 'is', 'in', 'full', 'support', 'of', 'blm', 'and', 'the', 'protests', 'going', 'on', 'i', 'just', 'felt', 'weird', 'for', 'a', 'while', 'not', 'asking', 'them', 'how', 'their', 'dad', 'is', 'because', 'i', 'm', 'in', 'support', 'of', 'defunding', 'and', 'radically', 'changing', 'the', 'law', 'enforcement', 'system']]\n[['i', 'm', 'a', 'white', 'girl', 'in', 'the', 'uk', 'who', 'is', 'in', 'a', 'more', 'privileged', 'situation', 'than', 'a', 'lot', 'of', 'people', 'right', 'now', 'my', 'family', 'aren', 't', 'well', 'off', 'but', 'we', 'live', 'in', 'reasonably', 'cheap', 'housing', 'and', 'because', 'my', 'mum', 'and', 'grandma', 'are', 'essential', 'workers', 'we', 'haven', 't', 'had', 'to', 'deal', 'with', 'financial', 'issues', 'for', 'my', 'part', 'i', 'was', 'working', 'from', 'home', 'as', 'a', 'personal', 'assistant', 'but', 'my', 'boss', 'has', 'taken', 'a', 'leave', 'of', 'absence', 'for', 'reasons', 'unrelated', 'to', 'the', 'virus', 'as', 'a', 'result', 'i', 'feel', 'like', 'i', 'should', 'be', 'trying', 'to', 'do', 'more', 'to', 'help', 'but', 'i', 'am', 'extremely', 'conflicted', 'i', 'think', 'what', 'happened', 'to', 'george', 'floyd', 'was', 'horrific', 'i', 'think', 'systematic', 'racism', 'is', 'a', 'problem', 'that', 'effects', 'the', 'majority', 'of', 'black', 'people', 'i', 'only', 'say', 'the', 'majority', 'because', 'i', 'have', 'read', 'accounts', 'from', 'people', 'who', 'claim', 'to', 'not', 'be', 'effected', 'by', 'it', 'and', 'i', 'think', 'it', 's', 'wrong', 'to', 'victimise', 'people', 'when', 'they', 'themselves', 'don', 't', 'think', 'they', 're', 'effected', 'by', 'it', 'however', 'i', 'strongly', 'disagree', 'with', 'all', 'the', 'people', 'who', 'think', 'all', 'cops', 'are', 'terrible', 'and', 'that', 'we', 'should', 'defund', 'the', 'police', 'i', 'do', 'think', 'the', 'whole', 'thing', 'needs', 'reform', 'because', 'it', 's', 'a', 'real', 'show', 'and', 'only', 'really', 'approve', 'of', 'the', 'protests', 'as', 'opposed', 'to', 'the', 'riots', 'so', 'i', 'm', 'not', 'sure', 'if', 'going', 'to', 'protests', 'is', 'a', 'good', 'idea', 'or', 'not', 'or', 'if', 'i', 'should', 'be', 'doing', 'as', 'my', 'mother', 'tells', 'me', 'and', 'looking', 'for', 'a', 'new', 'job', 'since', 'right', 'now', 'i', 'm', 'just', 'sat', 'home', 'killing', 'time', 'i', 'wanted', 'this', 'off', 'my', 'chest', 'just', 'because', 'i', 'm', 'honestly', 'really', 'stressed', 'out', 'just', 'not', 'knowing', 'what', 'to', 'do', 'to', 'help', 'and', 'yes', 'i', 'know', 'this', 'is', 'a', 'very', 'privileged', 'problem', 'i', 'have', 'but', 'hey', 'it', 's', 'call', 'off', 'my', 'chest', 'for', 'a', 'reason']]\n[['i', 'see', 'a', 'miscommunication', 'happening', 'right', 'now', 'with', 'the', 'term', 'defund', 'the', 'police', 'perhaps', 'the', 'saying', 'should', 'really', 'be', 'refund', 'the', 'police', 'as', 'in', 'restructure', 'or', 'give', 'me', 'a', 'refund']]\n[['the', 'problem', 'i', 'see', 'happening', 'is', 'that', 'the', 'american', 'right', 'is', 'spinning', 'this', 'as', 'the', 'left', 'wants', 'lawlessness', 'and', 'to', 'defund', 'and', 'abolish', 'the', 'police', 'entirely', 'seriously', 'just', 'turn', 'on', 'fox', 'trump', 'is', 'going', 'to', 'try', 'and', 'most', 'likely', 'succeed', 'at', 'getting', 're', 'elected', 'off', 'this', 'misrepresentation', 'btw', 'if', 'you', 'haven', 't', 'registered', 'to', 'vote', 'you', 'aren', 't', 'protesting', 'however', 'trumps', 'a', 'lying', 'cu', 'rag', 'and', 'the', 'actual', 'idea', 'is', 'really', 'that', 'law', 'enforcement', 'services', 'is', 'broken', 'and', 'needs', 'to', 'be', 're', 'funded', 'because', 'this', 'service', 'sucks', 'and', 'we', 'want', 'a', 'refund']]\n[['i', 'hate', 'when', 'people', 'say', 'all', 'lives', 'matter', 'all', 'lives', 'do', 'not', 'matter', 'police', 'lives', 'do', 'not', 'matter', 'i', 'don', 't', 'understand', 'why', 'people', 'want', 'to', 'defund', 'the', 'police', 'how', 'about', 'just', 'get', 'rid', 'of', 'them', 'altogether']]\n[['i', 'have', 'had', 'gad', 'for', 'years', 'due', 'to', 'an', 'epilepsy', 'disorder', 'although', 'it', 's', 'been', 'especially', 'severe', 'since', 'last', 'august', 'due', 'to', 'moving', 'to', 'a', 'new', 'city', 'and', 'starting', 'my', 'second', 'm', 'a', 'degree', 'around', 'many', 'hyper', 'validating', 'and', 'critical', 'social', 'justice', 'warriors', 'my', 'anxiety', 'went', 'into', 'panic', 'mode', 'at', 'various', 'points', 'during', 'these', 'classes', 'and', 'when', 'the', 'pandemic', 'happened', 'i', 'finally', 'realized', 'my', 'gad', 'would', 'keep', 'cycling', 'so', 'i', 'began', 'to', 'take', 'antidepressants', 'for', 'the', 'first', 'time', 'and', 'they', 'had', 'just', 'begun', 'to', 'change', 'the', 'direction', 'of', 'my', 'thinking', 'for', 'positive', 'in', 'conjunction', 'with', 'therapy', 'the', 'reactions', 'to', 'george', 'floyd', 's', 'death', 'and', 'the', 'protests', 'on', 'facebook', 'brought', 'a', 'new', 'level', 'of', 'anxiety', 'to', 'me', 'because', 'my', 'anxiety', 'has', 'a', 'habit', 'of', 'manifesting', 'internally', 'and', 'comparing', 'myself', 'to', 'the', 'world', 'i', 'began', 'to', 'notice', 'friends', 'on', 'facebook', 'angrily', 'talking', 'about', 'how', 'white', 'people', 'who', 'weren', 't', 'protesting', 'were', 'the', 'real', 'enemies', 'how', 'white', 'people', 'need', 'to', 'educate', 'themselves', 'by', 'digesting', 'academic', 'theories', 'how', 'we', 're', 'not', 'allowed', 'to', 'ask', 'black', 'people', 'about', 'systemic', 'inequalities', 'we', 'have', 'to', 'figure', 'it', 'out', 'for', 'ourselves', 'this', 'always', 'made', 'me', 'super', 'anxious', 'in', 'college', 'not', 'because', 'i', 'wasn', 't', 'open', 'to', 'learning', 'but', 'because', 'race', 'theories', 'felt', 'like', 'quantum', 'physics', 'to', 'me', 'i', 'became', 'especially', 'anxious', 'on', 'facebook', 'as', 'friends', 'started', 'openly', 'supporting', 'looting', 'and', 'rioting', 'as', 'a', 'way', 'to', 'eliminate', 'the', 'fascist', 'police', 'state', 'when', 'i', 'believe', 'there', 'are', 'many', 'cops', 'who', 'are', 'doing', 'their', 'job', 'and', 'also', 'that', 'many', 'of', 'the', 'looters', 'and', 'rioters', 'weren', 't', 'protesters', 'my', 'friends', 'didn', 't', 'seem', 'to', 'care', 'just', 'that', 'police', 'forces', 'were', 'defunded', 'businesses', 'destroyed', 'and', 'government', 'buildings', 'removed', 'at', 'all', 'costs', 'i', 'had', 'to', 'eventually', 'delete', 'my', 'facebook', 'app', 'because', 'my', 'nerves', 'were', 'going', 'haywire', 'and', 'to', 'remove', 'the', 'image', 'that', 'the', 'current', 'state', 'of', 'society', 'was', 'coming', 'after', 'me', 'not', 'white', 'people', 'just', 'me', 'and', 'that', 's', 'the', 'main', 'reason', 'i', 'was', 'so', 'anxious', 'i', 'm', 'just', 'an', 'average', '30', 'something', 'guy', 'who', 's', 'trying', 'to', 'start', 'a', 'career', 'and', 'doesn', 't', 'make', 'a', 'huge', 'amount', 'of', 'money', 'but', 'it', 'was', 'plaguing', 'my', 'mind', 'so', 'facebook', 'is', 'leaving', 'for', 'a', 'while']]\n[['blm', 'is', 'i', 'don', 't', 'even', 'know', 'what', 'to', 'make', 'of', 'that', 'site', 'their', 'only', 'apparent', 'policy', 'proposal', 'is', 'the', 'wholesale', 'defunding', 'of', 'police', 'departments', 'which', 'at', 'the', 'most', 'charitable', 'it', 's', 'strategically', 'foolish', 'to', 'say', 'nothing', 'of', 'the', 'wisdom', 'of', 'the', 'policy', 'itself', 'i', 'still', 'struggle', 'to', 'believe', 'that', 'this', 'is', 'the', 'actual', 'official', 'site', 'of', 'a', 'movement', 'that', 'portends', 'to', 'be', 'serious', 'about', 'bringing', 'about', 'change']]\n[['i', 'think', 'part', 'of', 'the', 'point', 'of', 'the', 'protests', 'is', 'that', 'they', 'are', 'civil', 'disobedience', 'which', 'means', 'not', 'obtaining', 'permits', 'and', 'not', 'working', 'with', 'police', 'or', 'the', 'government', 'the', 'point', 'is', 'to', 'disrupt', 'and', 'challenge', 'the', 'power', 'institutions', 'the', 'end', 'goals', 'the', 'blacklivesmatter', 'move', 'to', 'have', 'made', 'is', 'to', 'defund', 'and', 'ultimately', 'abolish', 'police', 'which', 'is', 'a', 'movement', 'support', 'by', 'activist', 'academics', 'and', 'other', 'organizations', 'while', 'the', 'salem', 'movement', 'is', 'for', 'changes', 'in', 'salem', 'it', 's', 'important', 'to', 'be', 'looking', 'to', 'what', 'the', 'larger', 'movement', 'is', 'and', 'how', 'we', 'can', 'tap', 'in', 'to', 'support', 'that']]\n[['uw', 'has', 'already', 'successfully', 'defunded', 'groups', 'like', 'wpirg', 'because', 'of', 'this', 'for', 'the', 'love', 'of', 'all', 'that', 'is', 'holy', 'please', 'think', 'twice', 'about', 'the', 'message', 'you', 're', 'sending', 'you', 'are', 'in', 'a', 'university', 'filled', 'with', 'minority', 'groups', 'that', 'are', 'not', 'the', 'group', 'being', 'talked', 'about', 'you', 'have', 'a', 'huge', 'population', 'of', 'engineering', 'and', 'comp', 'sci', 'students', 'who', 'don', 't', 'weigh', 'human', 'outrage', 'without', 'justifiable', 'evidence', 'into', 'an', 'equation', 'when', 'discussing', 'solutions', 'to', 'extremely', 'complex', 'issues', 'like', 'racial', 'discrimination']]\n[['current', 'mission', 'defunding', 'the', 'police', 'budget', 'and', 'giving', 'the', 'money', 'strictly', 'to', 'black', 'communities']]\n[['in', 'supporting', 'black', 'lives', 'matter', 'will', 'defunding', 'the', 'police', 'and', 'using', 'for', 'better', 'community', 'services', 'and', 'communication', 'work', 'any', 'examples']]\n[['agreed', 'defunding', 'is', 'a', 'huge', 'part', 'of', 'what', 'needs', 'to', 'happen', 'there', 'are', '911', 'situations', 'where', 'the', 'best', 'response', 'isn', 't', 'a', 'police', 'officer', 'or', 'at', 'the', 'very', 'least', 'a', 'social', 'worker', 'should', 'be', 'along', 'with', 'them', 'it', 's', 'not', 'about', 'funding', 'better', 'police', 'it', 's', 'about', 'shifting', 'funding', 'from', 'the', 'police', 'to', 'better', 'institutions']]\n[['supporting', 'trump', 'means', 'several', 'things', 'first', 'that', 'you', 'support', 'a', 'blatant', 'racist', 'who', 'has', 'weaponized', 'that', 'racism', 'against', 'multiple', 'ethnicities', 'travel', 'bans', 'from', 'middle', 'eastern', 'countries', 'defunding', 'black', 'communities', 'putting', 'mexican', 'children', 'in', 'cages', 'and', 'separating', 'them', 'from', 'their', 'own', 'families', 'you', 'claim', 'mark', 'calloway', 'is', 'a', 'good', 'dad', 'but', 'if', 'he', 'doesn', 't', 'see', 'the', 'problem', 'with', 'tearing', 'families', 'apart', 'like', 'that', 'i', 'can', 't', 'agree']]\n[['also', 'this', 'is', 'something', 'that', 'is', 'contested', 'and', 'debated', 'across', 'all', 'racial', 'lines', 'even', 'anti', 'racist', 'white', 'people', 'in', 'our', 'individual', 'and', 'joint', 'movements', 'can', 'we', 'decide', 'what', 'we', 're', 'fighting', 'for', 'is', 'it', 'defunding', 'and', 'reformation', 'is', 'it', 'facepalm', 'more', 'people', 'of', 'color', 'in', 'these', 'positions', 'of', 'power', 'please', 'no', 'or', 'is', 'the', 'complete', 'destruction', 'of', 'these', 'systems', 'of', 'times', 'past', 'hint', 'there', 's', 'only', 'one', 'right', 'answer', 'can', 'we', 'please', 'get', 'some', 'delegates', 'to', 'the', 'floor', 'only', 'partially', 'kidding', 'some', 'type', 'of', 'poc', 'coalition', 'there', 'needs', 'to', 'be', 'some', 'organization', 'i', 'know', 'they', 'existed', 'before', 'and', 'i', 'know', 'now', 'more', 'than', 'even', 'we', 'need', 'one', 'organize', 'it', 's', 'the', 'one', 'thing', 'holding', 'us', 'back', 'from', 'long', 'term', 'success', 'succinct', 'organization', 'across', 'racial', 'and', 'economic', 'lines', 'i', 'know', 'i', 'still', 'have', 'a', 'lot', 'more', 'to', 'cover', 'and', 'there', 's', 'some', 'scramble', 'here', 'that', 'braver', 'minds', 'can', 'better', 'explain', 'so', 'i', 'encourage', 'you', 'all', 'to']]\nPositive Posts : 32.6530612244898\nNegative Posts : 37.95918367346939\nNeutral Post : 29.38775510204082\n"
    }
   ],
   "source": [
    "#### Positive And Negative Counting #####\n",
    "\n",
    "positive_post = []\n",
    "negative_post = []\n",
    "neutral_post = []\n",
    "\n",
    "for data in data_list:\n",
    "\n",
    "    score = run_analysis(data)\n",
    "\n",
    "    if score > 0 : \n",
    "        positive_post.append(data)\n",
    "    elif score == 0 :\n",
    "        neutral_post.append(data)\n",
    "    else:\n",
    "        negative_post.append(data)\n",
    "\n",
    "# print(len(negative_post)/len(data_list)*100)\n",
    "\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "# for sentence in data_list:\n",
    "#     vs = analyzer.polarity_scores(sentence)\n",
    "    \n",
    "#     if vs[\"pos\"] > vs[\"neg\"] and vs[\"pos\"]>vs[\"neu\"]:\n",
    "#         positive_post.append(sentence)\n",
    "#     elif vs[\"neg\"] > vs[\"pos\"] and vs[\"neg\"] > vs[\"neu\"]:\n",
    "#         negative_post.append(sentence)\n",
    "#     else:\n",
    "#         neutral_post.append(sentence)\n",
    "    \n",
    "#     print(\"{:-<65} {}\".format(sentence, str(vs)))\n",
    "\n",
    "print(\"Positive Posts :\", len(positive_post)/len(data_list)*100)\n",
    "print(\"Negative Posts :\", len(negative_post)/len(data_list)*100)\n",
    "print(\"Neutral Post :\",len(neutral_post)/len(data_list)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['bernie marijuanders pledges support to blacklivesmatter movement says institutionalised prejudice and state brutality is unacceptable while awkwardly avoiding eye contact with palestine', 'serious non police emergency responders of reddit what is your opinion on topics like ferguson and police brutality', 'blacklivesmatter attempts to exploit edited arrest video to promote police brutality lenexa ks police force then releases full video of the arrest exposing the truth', 'blacklivesmatter attempts to exploit edited arrest video to promote police brutality lenexa ks police force then releases full video of the arrest exposing the truth', 'blacklivesmatter attempts to exploit edited arrest video to promote police brutality lenexa ks police force then releases full video of the arrest exposing the truth']\n"
    }
   ],
   "source": [
    "print(neutral_post[:5])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37364bitprogramdatavirtualenvfdc9b17c209e4355b8cd06321c70cf29",
   "display_name": "Python 3.7.3 64-bit ('ProgramData': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}